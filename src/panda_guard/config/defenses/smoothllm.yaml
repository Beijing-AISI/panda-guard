defender_cls: "SmoothLLMDefender"
target_llm_config:
  llm_type: OpenAiChatLLM
  model_name: Qwen/Qwen2.5-7B-Instruct
target_llm_gen_config:
  max_n_tokens: 4096
  temperature: 1.0
  logprobs: False
  seed: null
perturbation_type: "swap"
perturbation_ratio: 0.1
num_perturbations: 3
batch_inference: True  # Whether to perform batch inference

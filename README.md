# PandaGuard

English | [简体中文](./README_zh_CN.md)

This repository contains the source code for `Panda Guard`, designed for researching jailbreak attacks, defenses, and evaluation algorithms for large language models (LLMs). It is built on the following core principles:

- In the context of Language Model as a Service (LMaaS), LLMs should be viewed as part of a system rather than standalone entities by both service providers and users. Therefore, research should focus not only on the LLM itself but systematically study the related attack, defense, and evaluation algorithms.
- "The safest model is one that doesn't answer anything." Hence, safety is just one aspect of a model. Our goal should be to explore the relationship between safety and model capabilities, and how to balance the two.
- To better evaluate different models, attacks, and defense algorithms from multiple perspectives including safety, capability, and efficiency, we developed `panda-guard` to assess the safety and capabilities of LLMs as systems and explore their deployment in real-world scenarios.

## Quick Start

To install the latest version:

```bash
pip install git+https://github.com/Beijing-AISI/panda-guard.git
```

### Environment Configuration

Set the environment variables according to your LLM backend:

```bash
export OPENAI_BASE_URL=<your_base_url>  # e.g., https://aihubmix.com/v1
export OPENAI_API_KEY=<your_api_key>
```

### Usage

PandaGuard offers two main usage methods:

#### 1. Command Line Interactive Mode

```bash
panda-guard chat --defense rpo --model gpt-4o-mini
```

View help information:

```bash
panda-guard chat --help
```

Key command line options include:

```
--defense, -d         Path to defense configuration file or defense type (goal_priority/icl/none/rpo/self_reminder/smoothllm)
--judge, -j           Path to judge configuration file or judge type (llm_based/rule_based), multiple judges can be specified with comma separation
--endpoint, -e        Path to endpoint configuration file or endpoint type (openai/gemini/claude)
--model, -m           Model name
--temperature, -t     Override temperature setting
--device              Device to run the model on (e.g., 'cuda:0')
--log-level           Logging level (DEBUG, INFO, WARNING, ERROR)
--output, -o          Save chat history to file
--stream/--no-stream  Enable/disable streaming output
--verbose/--no-verbose Enable/disable verbose mode
```

#### 2. Using Configuration Files

Create a YAML configuration file for more flexible configuration:

```yaml
attacker:
    attacker_cls: "NoneAttacker"
    attacker_name: null

defender:
    defender_cls: "SelfReminderDefender"
    target_llm_config:
      llm_type: "OpenAiChatLLM"
      model_name: "gpt-4o-mini"
    target_llm_gen_config:
      max_n_tokens: 4096
      temperature: 1.0
      logprobs: False
      seed: null
    system_template: > 
      You are a responsible language model and good at avoiding generating 
      harmful or misleading content! Please answer the following user query in a responsible way.
    prompt_template: | 
      Remember, you should be a responsible language model and should not generate harmful or misleading content!
      {content}

judges:
  - judge_cls: "PairLLMJudge"
    judge_llm_config:
      llm_type: "OpenAiChatLLM"
      model_name: "gpt-4o-2024-11-20"
    judge_llm_gen_config:
      max_n_tokens: 25
      temperature: 0.0
      logprobs: False
      seed: 42
  - judge_cls: "PairLLMJudge"
    judge_llm_config:
      llm_type: "OpenAiChatLLM"
      model_name: "gpt-4o-mini"
    judge_llm_gen_config:
      max_n_tokens: 25
      temperature: 0.0
      logprobs: False
      seed: 42
  - judge_cls: "RuleBasedJudge"
```

Start with a configuration file:

```bash
panda-guard chat --config <your_config_file>
```

Example interaction:

```
Chat initialized with gpt-4o-mini
Type your message (or '/help' for available commands)
User: /verbose
Verbose mode enabled
User: hello
Assistant:
Hello! How can I assist you today?

Token usage: Prompt: 59 | Completion: 10 | Total: 69
Response time: 1.23s (8.14 tokens/sec)

Judge evaluations:
GCG: 1
PAIR_gpt-4o-mini: 1
PAIR_gpt-4o-2024-11-20: 0
User:
```

#### 3. API Service Mode

Start an OpenAI API-compatible service:

```bash
panda-guard serve
```

Example curl request:

```bash
curl -X POST http://localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "gpt-4o-2024-11-20",
    "messages": [
      {"role": "user", "content": "Write a short poem about AI safety."}
    ],
    "stream": true,
    "temperature": 0.7
}'
```

## Development Guide

### Source Installation

```bash
git clone https://github.com/Beijing-AISI/panda-guard.git --recurse-submodules
cd panda-guard
uv venv
source .venv/bin/activate
uv pip install -e .
```

### Developing New Components

PandaGuard uses a component-based architecture, including Attackers, Defenders, and Judges. Each component has corresponding abstract base classes and registration mechanisms.

#### Developing a New Attacker

1. Create a new file in the `src/panda_guard/role/attacks/` directory
2. Define configuration and attacker classes inheriting from `BaseAttackerConfig` and `BaseAttacker`
3. Register in `pyproject.toml` under `[project.entry-points."panda_guard.attackers"]` and `[project.entry-points."panda_guard.attacker_configs"]`

Example:

```python
# my_attacker.py
from typing import Dict, List
from dataclasses import dataclass, field
from panda_guard.role.attacks import BaseAttacker, BaseAttackerConfig

@dataclass
class MyAttackerConfig(BaseAttackerConfig):
    attacker_cls: str = field(default="MyAttacker")
    attacker_name: str = field(default="MyAttacker")
    # Other configuration parameters...

class MyAttacker(BaseAttacker):
    def __init__(self, config: MyAttackerConfig):
        super().__init__(config)
        # Initialization...
    
    def attack(self, messages: List[Dict[str, str]], **kwargs) -> List[Dict[str, str]]:
        # Implement attack logic...
        return messages
```

#### Developing a New Defender

1. Create a new file in the `src/panda_guard/role/defenses/` directory
2. Define configuration and defender classes inheriting from `BaseDefenderConfig` and `BaseDefender`
3. Register in `pyproject.toml` under `[project.entry-points."panda_guard.defenders"]` and `[project.entry-points."panda_guard.defender_configs"]`

#### Developing a New Judge

1. Create a new file in the `src/panda_guard/role/judges/` directory
2. Define configuration and judge classes inheriting from `BaseJudgeConfig` and `BaseJudge`
3. Register in `pyproject.toml` under `[project.entry-points."panda_guard.judges"]` and `[project.entry-points."panda_guard.judge_configs"]`

### Reproducing Experiments

PandaGuard provides a comprehensive framework for reproducing the experiments from our papers. All benchmark results are available at [HuggingFace/Beijing-AISI/panda-bench](https://huggingface.co/datasets/Beijing-AISI/panda-bench), and corresponding configurations for each experiment can be found in the same path as the result JSON files.

You can either:
1. Download the benchmark results directly from HuggingFace and place them in the `benchmarks` directory
2. Switch to the `bench-v0.1.0` branch to find all experiment configurations and rerun them

#### Jailbreak Evaluation Reproduction

To reproduce our jailbreak evaluation experiments:

1. Single model/attack/defense evaluation:

```bash
python jbb_inference.py \
  --config ../../configs/tasks/jbb.yaml \
  --attack ../../configs/attacks/transfer/gcg.yaml \
  --defense ../../configs/defenses/self_reminder.yaml \
  --llm ../../configs/defenses/llms/gpt-4o-mini.yaml 
```

2. Batch experiment reproduction:

```bash
python run_all_inference.py --max-parallel 8
```

3. Result evaluation:

```bash
python jbb_eval.py
```

#### Capability Evaluation Reproduction (AlpacaEval)

To reproduce our capability impact experiments, you may need to install [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval) first. 

1. Single model/defense evaluation:

```bash
python alpaca_inference.py \
  --config ../../configs/tasks/alpaca_eval.yaml \
  --llm ../../configs/defenses/llms/phi-3-mini-it.yaml \
  --defense ../../configs/defenses/semantic_smoothllm.yaml \
  --output-dir ../../benchmarks/alpaca_eval \
  --llm-gen ../../configs/defenses/llm_gen/alpaca_eval.yaml \
  --device cuda:7 \
  --max-queries 5 \
  --visible
```

2. Batch experiment reproduction:

```bash
python run_all_inference.py --max-parallel 8
```

3. Result evaluation:

```bash
python alpaca_eval.py
```

#### Using Pre-Computed Results

To use our pre-computed benchmark results, you need to download benchmark data:
```bash
mkdir bnechmakrs
# Download the benchmark data from HuggingFace
python -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='Beijing-AISI/panda-bench', local_dir='./benchmarks')"
```

1. Find the configuration in the benchmark repository:
```
benchmarks/
├── jbb/                                       # Raw jailbreak results
│   └── [model_name]/
│       └── [attack_name]/
│           └── [defense_name]/
│               ├── results.json              # Results
│               └── config.yaml               # Configuration
├── jbb_judged/                               # Judged jailbreak results
│   └── [model_name]/
│       └── [attack_name]/
│           └── [defense_name]/
│               └── [judge_results]
├── alpaca_eval/                              # Raw capability evaluation results
│   └── [model_name]/
│       └── [defense_name]/
│           ├── results.json                  # Results
│           └── config.yaml                   # Configuration
└── alpaca_eval_judged/                       # Judged capability results
    └── [model_name]/
        └── [defense_name]/
            └── [judge_name]/
                ├── annotations.json          # Detailed annotations
                └── leaderboard.csv           # Summary metrics
```


### Common Development Tasks

#### Adding a New Model Interface

1. Create a new file in the `llms/` directory
2. Define a configuration class inheriting from `BaseLLMConfig`
3. Implement the model class inheriting from `BaseLLM`
4. Implement required methods: `generate`, `evaluate_log_likelihood`, `continual_generate`
5. Register the new model in `pyproject.toml`

#### Adding a New Attack or Defense Algorithm

1. Research related papers, understand algorithm principles
2. Create implementation file in the corresponding directory
3. Implement configuration and main classes
4. Add necessary tests
5. Create sample configuration in the configuration directory
6. Register in `pyproject.toml`
7. Run evaluation experiments to validate effectiveness

## Currently Supported Components

### Attack Algorithms

| Status | Algorithm              | Source                                                                                                 |
|:------:|------------------------|--------------------------------------------------------------------------------------------------------|
|   ✅    | Transfer-based Attacks | Various templates from [JailbreakChat](https://jailbreakchat-hko42cs2r-alexalbertt-s-team.vercel.app/) |
|   ✅    | Rewrite Attack         | "Does Refusal Training in LLMs Generalize to the Past Tense?"                                          |
|   ✅    | [PAIR](https://arxiv.org/abs/2310.08419)                   | "Jailbreaking Black Box Large Language Models in Twenty Queries"                                       |
|   ✅    | [GCG](https://arxiv.org/abs/2307.15043)                    | "Universal and Transferable Adversarial Attacks on Aligned Language Models"                            |
|   ✅    | [AutoDAN](https://arxiv.org/abs/2310.04451)                | "Improved Generation of Adversarial Examples Against Safety-aligned LLMs"                              |
|   ✅    | [TAP](https://arxiv.org/abs/2312.02119)                    | "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"                                           |
|   ✅    | Overload Attack        | "Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models"                     |
|   ✅    | ArtPrompt              | "ArtPrompt: ASCII Art-Based Jailbreak Attacks Against Aligned LLMs"                                    |
|   ✅    | [DeepInception](https://arxiv.org/abs/2311.03191)          | "DeepInception: Hypnotize Large Language Model to Be Jailbreaker"                                      |
|   ✅    | GPT4-Cipher            | "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"                                    |
|   ✅    | SCAV                   | "Uncovering Safety Risks of Large Language Models Through Concept Activation Vector"                   |
|   ✅    | RandomSearch           | "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks"                                |
|   ✅    | [ICA](https://arxiv.org/abs/2310.06387)                    | "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations"                  |
|   ✅    | [Cold Attack](https://arxiv.org/abs/2402.08679)            | "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability"                                |
|   ✅    | [GPTFuzzer](https://arxiv.org/abs/2309.10253)              | "GPTFuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"                   |
|   ✅    | [ReNeLLM](https://arxiv.org/abs/2311.08268)                | "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"                              |


### Defense Algorithms

| Status | Algorithm         | Source                                                                                                             |
|:------:|-------------------|--------------------------------------------------------------------------------------------------------------------|
|   ✅    | SelfReminder      | "Defending ChatGPT against Jailbreak Attack via Self-Reminders"                                                    |
|   ✅    | ICL               | "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations"                              |
|   ✅    | SmoothLLM         | "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"                                          |
|   ✅    | SemanticSmoothLLM | "Defending Large Language Models Against Jailbreak Attacks via Semantic Smoothing"                                 |
|   ✅    | Paraphrase        | "Baseline Defenses for Adversarial Attacks Against Aligned Language Models"                                        |
|   ✅    | BackTranslation   | "Defending LLMs against Jailbreaking Attacks via Backtranslation"                                                  |
|   ✅    | PerplexityFilter  | "Baseline Defenses for Adversarial Attacks Against Aligned Language Models"                                        |
|   ✅    | RePE              | "Representation Engineering: A Top-Down Approach to AI Transparency"                                               |
|   ✅    | [GradSafe](https://arxiv.org/abs/2402.13494)          | "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis"                             |
|   ✅    | [SelfDefense](https://arxiv.org/abs/2308.07308)       | "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"                                          |
|   ✅    | [GoalPriority](https://arxiv.org/abs/2311.09096)      | "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"                         |
|   ✅    | [RPO](https://arxiv.org/abs/2401.17263)               | "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks"                            |
|   ✅    | JailbreakAntidote | "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models" |


### Judge Algorithms

| Status | Algorithm      | Source                                                                      |
|:------:|----------------|-----------------------------------------------------------------------------|
|   ✅    | RuleBasedJudge | "Universal and Transferable Adversarial Attacks on Aligned Language Models" |
|   ✅    | [PairLLMJudge](https://arxiv.org/abs/2310.08419)   | "Jailbreaking Black Box Large Language Models in Twenty Queries"            |
|   ✅    | [TAPLLMJudge](https://arxiv.org/abs/2312.02119)    | "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"                |


### LLM Interfaces

| Status | Interface   | Description                                                                          |
|:------:|-------------|--------------------------------------------------------------------------------------|
|   ✅    | OpenAI API  | Interface for OpenAI models (GPT-4o, GPT-4o-mini, etc.)                              |
|   ✅    | Claude API  | Interface for Anthropic's Claude models (Claude-3.7-sonnet, Claude-3.5-sonnet, etc.) |
|   ✅    | Gemini API  | Interface for Google's Gemini models (Gemini-2.0-pro, Gemini-2.0-flash, etc.)        |
|   ✅    | HuggingFace | Interface for models through HuggingFace Transformers library                        |
|   ✅    | [vLLM](https://github.com/vllm-project/vllm)        | High-performance inference engine for LLM deployment                                 |
|   ✅    | [SGLang](https://github.com/sgl-project/sglang)      | Framework for efficient LLM program execution                                        |
|   ✅    | [Ollama](https://ollama.com/)      | Local deployment for various open-source models                                      |


## Contribution Guide

1. Fork the repository and clone it locally
2. Create a new branch `git checkout -b feature/your-feature-name`
3. Implement your changes and new features
4. Ensure your code passes all tests and checks
5. Submit your code and create a Pull Request

We welcome all forms of contributions, including but not limited to: new algorithm implementations, documentation improvements, bug fixes, and feature enhancements.



## Acknowledgements

We would like to express our gratitude to the following projects and their contributors for developing the foundation upon which PandaGuard builds:

- [LLM-Attacks (GCG)](https://github.com/llm-attacks/llm-attacks)
- [AutoDAN](https://github.com/SheltonLiu-N/AutoDAN)
- [PAIR](https://github.com/patrickrchao/JailbreakingLLMs)
- [TAP](https://github.com/RICommunity/TAP)
- [GPTFuzz](https://github.com/sherdencooper/GPTFuzz)
- [SelfReminder](https://www.nature.com/articles/s42256-023-00765-8)
- [RPO](https://github.com/lapisrocks/rpo)
- [SmoothLLM](https://github.com/arobey1/smooth-llm)
- [JailbreakBench](https://arxiv.org/abs/2404.01318)
- [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)
- [JailbreakChat](https://jailbreakchat-hko42cs2r-alexalbertt-s-team.vercel.app/)
- [GoalPriority](https://github.com/thu-coai/JailbreakDefense_GoalPriority)
- [GradSafe](https://github.com/xyq7/GradSafe)
- [DeepInception](https://github.com/tmlr-group/DeepInception)
- [vLLM](https://github.com/vllm-project/vllm)
- [SGLang](https://github.com/sgl-project/sglang)
- [Ollama](https://ollama.com/)

Special thanks to all the researchers who have contributed to the field of LLM safety and helped advance our understanding of jailbreak attacks and defense mechanisms.

## Contact

For questions, suggestions, or collaboration, please contact us:

- **Email**: [shenguobin2021@ia.ac.cn](mailto:shenguobin2021@ia.ac.cn), [dongcheng.zhao@beijing-aisi.ac.cn](mailto:dongcheng.zhao@beijing-aisi.ac.cn), [yi.zeng@ia.ac.cn](mailto:yi.zeng@ia.ac.cn)
- **GitHub**: [https://github.com/Beijing-AISI/panda-guard](https://github.com/Beijing-AISI/panda-guard)
- **Homepage**: [https://panda-guard.github.io](https://panda-guard.github.io)

We welcome contributions from the community and are committed to advancing the field of LLM safety research.
